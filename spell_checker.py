import re
import sys
import random
import math
import collections
from collections import defaultdict
from collections import Counter


class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns amodel from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and caracter level.
    """
    n = 0
    chars = False
    model_dict = None

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Arges:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n = n
        self.chars = chars
        self.model_dict = defaultdict(int)  # Gram counter
        self.model_count_gram = {}  # Counter distribution from each gram to the next word/char

    def build_model(self, text):  # should be called build_model
        """populates a dictionary counting all ngrams in the specified text.

            Args:
                text (str): the text to construct the model from.
        """
        count = 0
        # Defer char model from words model using sep_char - in case of words use spaces between words
        if self.chars:
            grams = text
            sep_char = ''
        else:
            grams = text.split()
            sep_char = ' '
        for token in grams[:len(grams) - self.n + 1]:
            full_gram = sep_char.join(i for i in grams[count:count + self.n])
            # Count each gram (model_dict)
            self.model_dict[full_gram] += 1
            gram = sep_char.join(i for i in grams[count:count + self.n - 1])
            end = grams[count + self.n - 1]
            # Pre calculate the distribution from each gram to the next word/char [for exmaple - 3-gram: ['a cat']-> {'sat':2,'ate':4}]
            if not gram in self.model_count_gram:
                self.model_count_gram[gram] = Counter()
            self.model_count_gram[gram][end] += 1
            count = count + 1

    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
        """
        return self.model_dict

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        # In case of a unigram model all we need to do is sample n words according to their distribution
        if self.n == 1:
            sep_char = ' '
            if self.chars:
                sep_char = ''
            return sep_char.join(
                random.choices(list(self.model_dict.keys()), list(self.model_dict.values()))[0] for i in range(n))
        # If the context is none OR If the context is not in the model - generate a sentence without the context,
        # according to distributions
        if (not context in self.model_count_gram) or (not context):
            return self.generate_recursive(context, n)

        return context + self.generate_recursive(context, n)

    def generate_recursive(self, context=None, n=20):
        """Auxiliary function for generate

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        # Stop condition
        if n <= 0:
            return ''
        # When the context is null - we sample a context according to the model distribution
        if not context:
            context = random.choices(list(self.model_count_gram.keys()),
                                     [sum(i.values()) for i in self.model_count_gram.values()])[0]
            return context + self.generate_recursive(context, n - self.n + 1)
        # When context is not in the model- we sample a context according to a uniform distribution
        elif not context in self.model_count_gram:
            context = random.choices(list(self.model_count_gram.keys()))[0]
            return context + self.generate_recursive(context, n - self.n + 1)
        # Context in model - sample next word according to distribution
        elif context in self.model_count_gram:
            options = self.model_count_gram[context]
            sampled_word = random.choices(list(options.keys()), list(options.values()))[0]
            if self.chars:
                next_gram = ('' if context[1:] == [] else context[1:]) + sampled_word
                return sampled_word + self.generate_recursive(next_gram, n - 1)
            else:
                next_gram = ('' if context.split(' ')[1:] == [] else context.split(' ', 1)[1]) + (
                    ' ' if self.n > 2 else '') + sampled_word
                return ' ' + sampled_word + self.generate_recursive(next_gram, n - 1)
        else:
            # Senity check
            return ''

    def evaluate(self, text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to ebaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        eval = 1
        text = normalize_text(text)
        sep_char = ''
        need_smoothing = False
        # Check model type
        if self.chars:
            text = text
        else:
            text = text.split()
            sep_char = ' '
        # Unigram model
        if self.n == 1:
            words_count = sum(self.model_dict.values())
            for count in range(len(text)):
                # For each word we calculate the number of times she appeared devided by the count of all words and
                # multiply all together
                if text[count] in self.model_dict:
                    eval *= self.model_dict[text[count]] / words_count
                # Context does not exists - re-calculate from the beginning using smoothing
                else:
                    need_smoothing = True
                    break
        # N-gram model (N>1)
        else:
            for count in range(len(text[:len(text) - self.n + 1])):
                # For each gram(size N-1) we calculate the number of times this gram apeard with the n'th word,
                # devided by the count of this gram, multiply all together to a final evaluation
                gram = sep_char.join(i for i in text[count:count + self.n - 1])
                end = text[count + self.n - 1]
                if (gram in self.model_count_gram) and (self.model_count_gram[gram][end] != 0):
                    eval = eval * self.model_count_gram[gram][end] / sum(self.model_count_gram[gram].values())
                # Context does not exists - re-calculate from the beginning using smoothing
                else:
                    need_smoothing = True
                    break
        # Re-calculate using smoothing
        if need_smoothing:
            eval = 1
            for count in range(len(text[:len(text) - self.n + 1])):
                eval = eval * self.smooth(text[count:count + self.n])

        return math.log(eval)

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        sep_char = ' '
        if self.chars:
            sep_char = ''
        # Smoothing- add 1 to counter, add total number of (N-1)-grams (unique) to denominator
        if self.n > 1:
            gram = sep_char.join(i for i in ngram[:self.n - 1])
            end = ngram[self.n - 1]
            if (gram in self.model_count_gram) and (self.model_count_gram[gram][end] != 0):
                return (self.model_count_gram[gram][end] + 1) / (
                            sum(self.model_count_gram[gram].values()) + len(self.model_count_gram.keys()))  # smoothing
            else:
                return 1 / len(self.model_count_gram.keys())
        else:
            # Unigram
            words_count = sum(self.model_dict.values())
            unique = len(self.model_dict.keys())
            if ngram[0] in self.model_dict:
                return (self.model_dict[ngram[0]] + 1) / (words_count + unique)
            else:
                return 1 / (words_count + unique)


class Error_Learner:
    """The class implements a learning process of error​​ ​distribution tables.
    """
    map_inx_to_direction = {0: 'substitution', 1: 'insertion', 2: 'deletion'}
    map_direction_to_idx = {'substitution': 0, 'insertion': 1, 'deletion': 2}

    def get_change(self, error_type, error_char, prev_correct_char, correct_char):
        """Returns the error to add according to the error_type.
                  Args:
                      error_type (str): insertion/deletion/substitution
                      error_char,prev_correct_char, correct_char (char) : the needed chars for the error learning

                  Returns:
                      The pair of letters that need to be added to the error model
        """
        if error_type == 'insertion':
            ret = correct_char + error_char
        elif error_type == 'deletion':
            ret = prev_correct_char + correct_char
        else:
            ret = error_char + correct_char
        return ret

    def retrieve_path(self, w_correct, w_error, error_len, correct_len, dist, dist_path, error_tables):
        """Adding the error chars to the error list according to the error type
                  Args:
                      w_correct, w_error (str): the correct word and the error word
                      error_len, correct_len (int) : the correct word and the error word len
                      dist, dist_path (multi-dim array) : build in min_distance using dynamic programming. using
                                                          dist_path we can retrieve the shortest path to the correction
                      error_tables (dict): the error table to add the error to
        """
        i = error_len - 1
        j = correct_len - 1
        prev = dist[i][j]
        substitution_sign = False
        substitution_chars = None
        counter = 0
        # while we are not at the beginning point of the table
        while prev:
            counter += 1
            error_char = w_error[i]
            prev_correct_char = w_correct[j - 1]
            correct_char = w_correct[j]
            dir = self.map_inx_to_direction[dist_path[i][j]]
            # move to the next cell according to direction - substitution= diagonal, deletion = left,  insertion = up
            if dir == 'substitution':
                j = j - 1
                i = i - 1
            elif dir == 'deletion':
                j = j - 1  # deletion
            else:
                i = i - 1  # insertion
            curr = dist[i][j]
            if prev > curr:  # change- save the correction and move on to the next correction
                prev = curr
                error_chars = self.get_change(dir, error_char, prev_correct_char, correct_char)
                if dir != 'substitution':
                    substitution_sign = False
                    if error_chars in error_tables[dir]:
                        error_tables[dir][error_chars] += 1
                    else:
                        error_tables[dir][error_chars] = 1
                else:
                    # instead of two substitution errors, change to one transposition
                    if substitution_sign and (error_chars == substitution_chars[::-1]):
                        if substitution_chars in error_tables['transposition']:
                            error_tables['transposition'][substitution_chars] += 1
                        else:
                            error_tables['transposition'][substitution_chars] = 1
                        error_tables['substitution'][substitution_chars] -= 1
                        substitution_sign = False
                    else:
                        # save the error chars, remove next round in case that's part of a transposition
                        if error_chars in error_tables[dir]:
                            error_tables[dir][error_chars] += 1
                        else:
                            error_tables[dir][error_chars] = 1
                        substitution_chars = error_chars
                        substitution_sign = True

    def min_distance(self, w_error, w_correct, error_tables):
        """Looking for the min edit distance from the error word to the correct word
        using dynamic programming
                  Args:
                      w_correct, w_error (str): the correct word and the error word
                      error_tables (dict): the error table to add the error to
        """
        w_correct = '#' + w_correct
        w_error = '#' + w_error
        correct_len = len(w_correct)
        error_len = len(w_error)
        # create two-dim matrix error_len X correct_len for dynamic programming and retrieve the shortest path
        dist = [[0 for c in range(correct_len)] for r in range(error_len)]
        dist_path = [[0 for c in range(correct_len)] for r in range(error_len)]  # error_len X correct_len - update path
        # look for the shortest correction using dynamic programming
        # initialize dist_path
        for i in range(1, error_len):
            dist_path[i][0] = self.map_direction_to_idx[
                'insertion']  # from the first line you can only move left-> which means insertion
        for j in range(1, correct_len):
            dist_path[0][j] = self.map_direction_to_idx[
                'deletion']  # from the first column you can only move right-> which means deletion
        # initialize dist according to the dynamic programming algorithm
        for i in range(1, error_len):
            dist[i][0] = dist[i - 1][0] + 1
        for j in range(1, correct_len):
            dist[0][j] = dist[0][j - 1] + 1
        for i in range(1, error_len):
            for j in range(1, correct_len):
                # look for the shortest option
                l = [dist[i - 1][j - 1] + (0 if w_error[i] == w_correct[j] else 1)  # substitution
                    , dist[i - 1][j] + 1  # insertion
                    , dist[i][j - 1] + 1]  # deletion
                dist_path[i][j] = l.index(min(l))  # save path
                dist[i][j] = l[dist_path[i][j]]
        # retrieve the shortest path
        self.retrieve_path(w_correct, w_error, error_len, correct_len, dist, dist_path, error_tables)
        return dist[error_len - 1][correct_len - 1]  # return changes counter


class Spell_Checker:
    """The class implements a context sensitive spell checker. The corrections
        are done in the Noisy Channel framework, based on a language model and
        an error distribution model.
    """
    error_tables = {'insertion': {}, 'deletion': {}, 'substitution': {},
                    'transposition': {}}  # error tables for the spelling correction prob
    spell_corrector = None  # spell corrector object
    unigram_counter = Counter()  # words counter to look see if correction candidate is relevant
    chars2_counter = Counter()  # sequences counter of all two characters in the lm, for the noisy channel prob ('aa': 5, 'bt':7...)
    chars_counter = Counter()  # counter of all characters in the lm, for the noisy channel prob ('a':5, 'f':9...)
    lm = None  # language model
    n = None  # lm n

    def __init__(self, lm=None):
        """Initializing a spell checker object with a language model as an
        instance  variable. The language model should suppport the evaluate()
        and the get_model() functions as defined in assignment #1.

        Args:
            lm: a language model object. Defaults to None
        """
        if lm:
            self.add_language_model(lm)

    def build_model(self, text, n=3):
        """Returns a language model object built on the specified text. The language
            model should support evaluate() and the get_model() functions as defined
            in assignment #1.

            Args:
                text (str): the text to construct the model from.
                n (int): the order of the n-gram model (defaults to 3).

            Returns:
                A language model object
        """
        text_norm = normalize_text(text)
        self.n = n
        self.lm = Ngram_Language_Model(n)
        self.lm.build_model(text_norm)
        self.unigram_counter = Counter()
        self.chars2_counter = Counter()
        self.chars_counter = Counter()
        if n == 1:
            self.unigram_counter = self.lm.get_model()
            # count all sequences of chars size 2 and 1 using unigram
            for token in self.unigram_counter:
                for i in range(len(token) - 1):
                    self.chars2_counter[token[i:i + 2]] += self.unigram_counter[token]
                    self.chars_counter[token[i]] += self.unigram_counter[token]
        else:
            # count all sequences of chars size 2 and 1 while creating unigram counter for prob
            n_gram_model = self.lm.get_model()
            for gram in n_gram_model:
                # split token into words and count words and chars for noisy channel
                words = gram.split()
                # for each token of the gram
                for token in words:
                    self.unigram_counter[token] += n_gram_model[gram]
                    for i in range(len(token) - 1):
                        self.chars2_counter[token[i:i + 2]] += n_gram_model[gram]
                        self.chars_counter[token[i]] += n_gram_model[gram]
        return self.lm

    def add_language_model(self, lm):
        """Adds the specified language model as an instance variable.
            (Replaces an older LM disctionary if set)

            Args:
                ls: a language model object
        """
        #sanity check
        if lm:
            self.lm = lm
            n_gram_model = lm.get_model()
            self.n = len(list(n_gram_model)[0].split())
            if self.n == 1:
                #update unigram counter according to the unigram
                self.unigram_counter = self.lm
            else:
                # update unigram counter using the lm
                n_gram_model = self.lm.get_model()
                for gram in n_gram_model:
                    words = gram.split()
                    #for each gram, split into tokens and count
                    for token in words:
                        self.unigram_counter[token] += n_gram_model[gram]
                        for i in range(len(token) - 1):
                            self.chars2_counter[token[i:i + 2]] += n_gram_model[gram]
                            self.chars_counter[token[i]] += n_gram_model[gram]

    def learn_error_tables(self, errors_file):
        """Returns a nested dictionary {str:dict} where str is in:
            <'deletion', 'insertion', 'transposition', 'substitution'> and the
            inner dict {str: int} represents the confution matrix of the
            specific errors, where str is a string of two characters matching the
            row and column "indixes" in the relevant confusion matrix and the int is the
            observed count of such an error (computed from the specified errors file).
            Examples of such string are 'xy', for deletion of a 'y'
            after an 'x', insertion of a 'y' after an 'x'  and substitution
            of 'x' (incorrect) by a 'y'; and example of a transposition is 'xy' indicates the characters that are transposed.

            ## Using Error_Learner class I learned the error tables ##

            Notes:
                1. Ultimately, one can use only 'deletion' and 'insertion' and have
                    'substitution' and 'transposition' derived. Again,  we use all
                    four types explicitly in order to keep things simple.
            Args:
                errors_file (str): full path to the errors file. File format, TSV:
                                    <error>    <correct>


            Returns:
                A dictionary of confusion "matrices" by error type (dict).
          """
        error_learner_ins = Error_Learner()
        try:
            errors_f = open(errors_file, "r")
        except IOError:
            return None # file doesnt exists
        # leran from each line
        for line in errors_f:
            try:
                w_error, w_correct = line.split()[0], line.split()[1]
                w_error, w_correct = line.split()[0], line.split()[1]
                error_learner_ins.min_distance(w_error, w_correct, self.error_tables)
            except:
                pass
        return self.error_tables

    def add_error_tables(self, error_tables):
        """ Adds the speficied dictionary of error tables as an instance variable.
            (Replaces an older value disctionary if set)

            Args:
                error_tables (dict): a dictionary of error tables in the format
                returned by  learn_error_tables()
        """
        self.error_tables = error_tables

    def evaluate(self, text):
        """Returns the log-likelihod of the specified text given the language
            model in use. Smoothing is applied on texts containing OOV words

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        if self.lm:
            return self.lm.evaluate(text)
        return None

    def noisy_channel_prob(self, error_tables_word):
        """ Calculatin the noist channel prob using the lm distribution
            Args:
                error_tables_word = {'insertion':{}, 'deletion':{}, 'substitution':{}, 'transposition':{}} - detailed error in a word
            Return:
                noisy channel prob
        """
        prob = 1
        if error_tables_word:
            # for each error insertion/deletion/substitution/transposition
            for error_type in error_tables_word:
                for error in error_tables_word[error_type]:  # for each specific error
                    for i in range(error_tables_word[error_type][error]):  # in case the same error happend twice
                        method = getattr(self, error_type + '_calc', lambda: "Invalid name")
                        prob *= method(error) # get prob
        return prob

    def insertion_calc(self, chars):
        """   Auxiliary function for noisy_channel_prob """
        prob = 0
        if chars in self.error_tables['insertion'] and chars[0] in self.chars_counter:
            prob = self.error_tables['insertion'][chars] / self.chars_counter[chars[0]]
        return prob

    def deletion_calc(self, chars):
        """   Auxiliary function for noisy_channel_prob """
        prob = 0
        if chars in self.error_tables['deletion'] and chars in self.chars2_counter:
            prob = self.error_tables['deletion'][chars] / self.chars2_counter[chars]
        return prob

    def substitution_calc(self, chars):
        """   Auxiliary function for noisy_channel_prob """
        prob = 0
        if chars in self.error_tables['substitution'] and chars[1] in self.chars_counter:
            prob = self.error_tables['substitution'][chars] / self.chars_counter[chars[1]]
        return prob

    def transposition_calc(self, chars):
        """   Auxiliary function for noisy_channel_prob """
        prob = 0
        if chars in self.error_tables['transposition'] and chars in self.chars2_counter:
            prob = self.error_tables['transposition'][chars] / self.chars2_counter[chars]
        return prob

    def spell_check(self, text, alpha):
        """ Returns the most probable fix for the specified text. Use a simple
            noisy channel model is the number of tokens in the specified text is
            smaller than the length (n) of the language model.

            Args:
                text (str): the text to spell check.
                alpha (float): the probability of keeping a lexical word as is.

            Return:
                A modified string (or a copy of the original if no corrections are made.)
        """
        if not (text and alpha and self.lm):
            return text
        error_learn_ins = Error_Learner()
        text_norm = normalize_text(text)
        text_split = text_norm.split()
        max_option_by_loc = [None]*len(text_split) # max possibility for each word
        noisy_channel_prob_channel = [None]*len(text_split)  # noisy channel prob for each best option word
        channel_model_sum_prob = 0  # sum prob for noisy channel normalization
        text_smaller_than_n = False
        temp_self_n = self.n
        eval_to_use = self.evaluate
        # sentence is too short, use simple noisy channel - temporary change the eval function to eval using simple unigram
        if len(text_split) < self.n:
            text_smaller_than_n = True
            eval_to_use = self.evaluate_gram_smaller_than_n
            self.n = 1
        # look for optional correction for each word in two edit distance
        for idx, word in enumerate(text_split):
            #for each word look for corrections
            if re.search('[a-zA-Z]', word): # if its a word not just signs
                max_possibility, max_noisy_channel, sum_noisy = self.find_best_option_for_word_correction(word, text_split, idx, error_learn_ins, eval_to_use)
                channel_model_sum_prob += sum_noisy
            else:
                max_possibility = None
                max_noisy_channel = 0
            max_option_by_loc[idx] = max_possibility  # save best option for idx
            noisy_channel_prob_channel[idx] = max_noisy_channel
        best_opt =  self.choose_best_option(max_option_by_loc, text_split, alpha, noisy_channel_prob_channel,
                                       channel_model_sum_prob, eval_to_use)
        if text_smaller_than_n:
            self.n = temp_self_n # retrieve n in case we switched to unigram calculation
        return best_opt

    def evaluate_gram_smaller_than_n(self, sentence):
        """
        calculate prob as unigram model with smoothing
        this function is only for cases where the sentence we need to fix is shorter than the lm we have
            Args:
                sentence (str): sentence to eval
            Return:
               unigram prob for this sentence
               this function is used when the sentence we need correct is shorter than the
               n-lm we have
               I didn't create a unigram model at the beginning to avoid long running time!!
        """
        if not sentence or not self.unigram_counter:
            return 0
        sum_words = sum(self.unigram_counter.values())
        unique = len(self.unigram_counter)
        prob = 1
        words = sentence.split()
        for word in words:
            if word in self.unigram_counter:
                prob *= self.unigram_counter[word]/sum_words
            else:
                prob *= 1 / (sum_words + unique) # smooth
        return prob

    def find_best_option_for_word_correction(self, word, text_split, idx, error_learn_ins, eval_to_use):
        """
            Args:
                word (str): the word we are trying to correct in a sentence
                text_split(array str): the sentence splited
                idx(int): the word index in the array
                error_learn_ins: error learner instance
                eval_to_use: function to use when evaluating
            Return:
                max_possibility: the chosen word with the highest prob
                max_noisy_channel: the noisy channel word for the word (to avoid re-calculate)
                channel_model_sum_prob: adding the probs from the for loop, for future normalization
        """
        correction_options = one_edits_away(word)  # create one edit distance list
        correction_options = set(
            correction_options.union(two_edits_away(correction_options)))  # create one+ two edit distance list
        correction_options.remove(word)  # remove the word itself to avoid the original sentence
        max_prob = 0
        max_noisy_channel = 0
        channel_model_sum_prob = 0
        max_possibility = None
        sentence_beginning = (' '.join(text_split[max(0, idx - self.n + 1):idx]) + ' ' if text_split[max(0,
                                                                                                         idx - self.n + 1):idx] else '')  # cut sentence to the relevent grams
        sentence_end = ' ' + ' '.join(text_split[idx + 1:])  # + (' ' if text_split[idx + 1:] else '')
        for option in correction_options:
            # for each option calculate prob using lm eval + noisy channel
            if option in self.unigram_counter:
                option_prob = 0
                error_tables_word = {'insertion': {}, 'deletion': {}, 'substitution': {}, 'transposition': {}}
                error_learn_ins.min_distance(word, option, error_tables_word)
                noisy_prob = self.noisy_channel_prob(error_tables_word)
                channel_model_sum_prob += noisy_prob
                option_prob = math.exp(eval_to_use(
                    sentence_beginning + option + sentence_end)) * noisy_prob  # evaluate just the grams related to this word
                if max_prob < option_prob: # save best option
                    max_prob = option_prob
                    max_possibility = option
                    max_noisy_channel = noisy_prob
        return max_possibility, max_noisy_channel, channel_model_sum_prob

    def choose_best_option(self, max_option_by_loc, text_split, alpha, noisy_channel_prob_channel,
                           channel_model_sum_prob, eval_to_use):
        """ After choosing best candidates for each word, choose the best sentence, considering the whole sentence and not just the grams
            Args:
                max_option_by_loc (list str):  best substitute for each word
                text_split (list str): the original sentence splitted to a list
                alpha (float): the probability of keeping a lexical word as is.
                noisy_channel_prob_channel (array): noisy channel prob for each word in max_option_by_loc
                channel_model_sum_prob (int): sum of channel model probabilities for normalization
                 eval_to_use: function to use when evaluating

            Return:
                A modified string (or a copy of the original if no corrections are made.)
        """
        max_prob = 0
        max_possibility_sentence = None
        if self.lm:
            for idx, word in enumerate(text_split):
                if max_option_by_loc[idx]: # if there is a correction to check
                    sentence_beginning = (' '.join(text_split[:idx]) + ' ' if text_split[:idx] else '')
                    sentence_end = (' '+' '.join(text_split[idx + 1:]) if idx < (len(text_split)-1) else '')
                    full_sentence = sentence_beginning + max_option_by_loc[idx] + sentence_end
                    option_prob = math.exp(eval_to_use(full_sentence)) * noisy_channel_prob_channel[
                        idx]  # evaluate sentence
                    if max_prob < option_prob: # save best
                        max_prob = option_prob
                        max_possibility_sentence = full_sentence
            orig_prob = math.exp(eval_to_use(' '.join(text_split))) * alpha
            if channel_model_sum_prob:
                max_prob = max_prob / (channel_model_sum_prob / (1 - alpha))
            if orig_prob > max_prob: # compare to original
                return ' '.join(text_split)
        return max_possibility_sentence

def normalize_text(text, remove_signs = False):
    """Returns a normalized string based on the specifiy string.
       You can add default parameters as you like (they should have default values!)
       You should explain your decitions in the header of the function.

       Args:
           text (str): the text to normalize

       Returns:
           string. the normalized text.
    """
    if text:
        text = text.lower()
        text = re.sub('([.,!?()])', r' \1 ', text)  # add padding
        text = re.sub('\s{2,}', ' ', text)  # remove more than two spaces
        if remove_signs:
            text = re.sub('[!#.?$]', '', text)
    return text


def one_edits_away(word):
    """Look for all possible edits that are one edit away from a word

        Args:
            word (str)
        Returns:
        list of possible words
  """
    if word:
        letters = 'abcdefghijklmnopqrstuvwxyz'
        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
        deletes = [L + R[1:] for L, R in splits if R]
        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]
        inserts = [L + c + R for L, R in splits for c in letters]
        return set(deletes + transposes + replaces + inserts)
    return None


def two_edits_away(word_one_edit):
    """Look for all possible edits that are two edit away from a word- word already one edit away
        Args:
            word (str)
        Returns:
        list of possible words
  """
    if word_one_edit:
        return (e2 for e1 in word_one_edit for e2 in one_edits_away(e1))
    return None
