import re
import sys
import random
import math
import collections
from collections import defaultdict
from collections import Counter

"""# Ngram Code"""

class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns amodel from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and caracter level.
    """
    n = 0
    chars = False
    model_dict = None
    model_count_gram = None

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Arges:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n = n
        self.chars = chars
        self.model_dict = defaultdict(int) # Gram counter
        self.model_count_gram = {} # Counter distribution from each gram to the next word/char

    def build_model(self, text):  #should be called build_model
        """populates a dictionary counting all ngrams in the specified text.

            Args:
                text (str): the text to construct the model from.
        """
        count = 0
        # Defer char model from words model using sep_char - in case of words use spaces between words
        if self.chars:
          grams = text
          sep_char = ''  
        else: 
          grams = text.split()
          sep_char = ' '
        for token in grams[:len(grams)-self.n + 1]:
          full_gram = sep_char.join(i for i in grams[count:count + self.n])
          # Count each gram (model_dict)
          self.model_dict[full_gram] += 1
          gram = sep_char.join(i for i in grams[count:count + self.n-1])
          end = grams[count + self.n-1]
          # Pre calculate the distribution from each gram to the next word/char [for exmaple - 3-gram: ['a cat']-> {'sat':2,'ate':4}]
          if not gram in self.model_count_gram:
            self.model_count_gram[gram] = Counter()
          self.model_count_gram[gram][end] += 1
          count = count + 1 

    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
        """ 
        return self.model_dict

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        # In case of a unigram model all we need to do is sample n words according to their distribution
        if self.n == 1:
          sep_char = ' '
          if self.chars: 
            sep_char = ''
          return sep_char.join(random.choices(list(self.model_dict.keys()),list(self.model_dict.values()))[0] for i in range(n))
        # If the context is none OR If the context is not in the model - generate a sentence without the context, according to distributions 
        if (not context in self.model_count_gram) or (not context): 
          return self.generate_recursive(context, n)

        return context + self.generate_recursive(context, n)

    def generate_recursive(self, context=None, n=20):
        """Auxiliary function for generate

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        # Stop condition
        if n <= 0:
          return ''
        # When the context is null - we sample a context according to the model distribution
        if not context:
          context = random.choices(list(self.model_count_gram.keys()),[sum(i.values()) for i in self.model_count_gram.values()] )[0]
          return context + self.generate_recursive(context, n - self.n + 1)
        # When context is not in the model- we sample a context according to a uniform distribution
        elif not context in self.model_count_gram:
          context = random.choices(list(self.model_count_gram.keys()))[0]
          return context + self.generate_recursive(context, n - self.n + 1)
        # Context in model - sample next word according to distribution
        elif context in self.model_count_gram:
          options = self.model_count_gram[context]
          sampled_word = random.choices(list(options.keys()),list(options.values()))[0]
          if self.chars:
            next_gram = ('' if context[1:] == [] else context[1:]) + sampled_word
            return sampled_word + self.generate_recursive(next_gram, n-1)
          else:
            next_gram = ('' if context.split(' ')[1:] == [] else  context.split(' ', 1)[1]) + (' ' if self.n > 2 else '') + sampled_word
            return ' ' + sampled_word + self.generate_recursive(next_gram, n-1)
        else:
          # Senity check
          return '' 


    def evaluate(self,text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to ebaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        eval = 1
        text = normalize_text(text)
        sep_char = '' 
        need_smoothing = False
        # Check model type
        if self.chars:
          text = text
        else: 
          text = text.split()
          sep_char = ' '
        # Unigram model
        if self.n == 1:
          words_count = sum(self.model_dict.values())
          for count in range(len(text)):
            # For each word we calculate the number of times she appeared devided by the count of all words and multiply all together
            if text[count] in self.model_dict:
              eval *= self.model_dict[text[count]]/words_count
            # Context does not exists - re-calculate from the beginning using smoothing
            else:
              need_smoothing = True
              break
        # N-gram model (N>1)
        else:
          for count in range(len(text[:len(text)-self.n + 1])):
            # For each gram(size N-1) we calculate the number of times this gram apeard with the n'th word, devided by the count of this gram, multiply all together to a final evaluation
            gram = sep_char.join(i for i in text[count:count + self.n-1])
            end = text[count + self.n-1]
            if (gram in self.model_count_gram) and (self.model_count_gram[gram][end] != 0):
              eval = eval*self.model_count_gram[gram][end]/sum(self.model_count_gram[gram].values())
            # Context does not exists - re-calculate from the beginning using smoothing
            else:
              need_smoothing = True
              break
        # Re-calculate using smoothing
        if need_smoothing: 
          eval = 1
          for count in range(len(text[:len(text)-self.n + 1])):
            eval = eval * self.smooth(text[count:count + self.n])

        return math.log(eval)

          


    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        sep_char = ' ' 
        if self.chars:
          sep_char = ''
        # Smoothing- add 1 to counter, add total number of (N-1)-grams (unique) to denominator
        if self.n > 1 :
          gram = sep_char.join(i for i in ngram[:self.n-1])
          end = ngram[self.n-1]
          if (gram in self.model_count_gram) and (self.model_count_gram[gram][end] != 0):
           return (self.model_count_gram[gram][end] + 1) / (sum(self.model_count_gram[gram].values()) + len(self.model_count_gram.keys())) # smoothing
          else:
            return 1/len(self.model_count_gram.keys())
        else:
          # Unigram
          words_count = sum(self.model_dict.values())
          unique = len(self.model_dict.keys())
          if ngram[0] in self.model_dict:
             return (self.model_dict[ngram[0]] + 1)/(words_count + unique)
          else:
            return 1/(words_count + unique)


def normalize_text(text):
    """Returns a normalized string based on the specifiy string.
       You can add default parameters as you like (they should have default values!)
       You should explain your decitions in the header of the function.

       Args:
           text (str): the text to normalize

       Returns:
           string. the normalized text.
    """
    text = text.lower()
    text = re.sub('([.,!?()])', r' \1 ', text) # Add padding
    text = re.sub('\s{2,}', ' ', text) # Remove more than two spaces
    return text
